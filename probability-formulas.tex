%\magnification=1200
\nopagenumbers
\parindent=0pt
\vsize=10in
\hsize=7.5in
\hoffset=-0.5in
\voffset=-0.5in

A collection $\cal F$ of subsets of $\Omega$ is a field
(i) if $A,B\in\cal F$ then $A\cup B\in\cal F$ and $A\cap B\in\cal F$,
(ii) if $A\in\cal F$ then $A^c\in\cal F$,
(iii) $\emptyset\in\cal F$.
$\sigma$-field:
(i) $\emptyset\in\cal F$,
(ii) if $A_1$, $A_2$, $\ldots\in\cal F$ then
$\cup_{i=1}^\infty A_i\in\cal F$,
(iii) if $A\in\cal F$ then $A^c\in\cal F$.

\smallskip
\hrule

\smallskip
$A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$
$\quad(A\cup B)^c=A^c\cap B^c$
$\quad(A\cap B)^c=A^c\cup B^c$
$\quad A\cup B=(A\cap B^c)\cup(A^c\cap B)\cup(A\cap B)$

\smallskip
$A\cap B^c=A-(A\cap B)$

\smallskip
\hrule

\smallskip
$P(\emptyset)=0$
$\quad P(\Omega)=1$
$\quad P(A^c)=1-P(A)\quad$
If $A\subseteq B$ then $P(A)\le P(B)$.

\smallskip
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$
$\quad P(A\cap B^c)=P(A)-P(A\cap B)=P(A-B)$

\smallskip
If $A_i$ are disjoint then
$\displaystyle P\left(\bigcup_{i=1}^\infty A_i\right)
=\sum_{i=1}^\infty P(A_i)$.
In general
$\displaystyle P\left(\bigcup_i A_i\right)\le\sum_i P(A_i)$.

\smallskip
\hrule

\smallskip
$\displaystyle P(A\mid B)={P(A\cap B)\over P(B)}$
$\quad P(A)=P(A\mid B)P(B)+P(A\mid B^c)P(B^c)$
$\quad P(C\mid A\cap B)P(A\cap B)=P(C\mid A\cap B)P(B\mid A)P(A)$

\smallskip
$\displaystyle P(ABC)=P(A\cap B\cap C)
={P(A\cap B\cap C)\over P(A\cap B)}P(A\cap B)
=P(C\mid A\cap B)P(A\cap B)$

\smallskip
$P(C)=P(C\mid A\cap B)P(A\cap B)
+P(C\mid A\cap B^c)P(A\cap B^c)
+P(C\mid A^c\cap B)P(A^c\cap B)
+P(C\mid A^c\cap B^c)P(A^c\cap B^c)$

\smallskip
Bayes' formula
$\displaystyle P(A\mid X)={P(X\mid A)P(A)\over
P(X\mid A)P(A)+P(X\mid B)P(B)+P(X\mid C)P(C)}$

\smallskip
Independence
$\quad P(A\cap B)=P(A)P(B)$
$\quad P(A\mid B)=P(A)$

\smallskip
Conditional independence
$\quad P(A\cap B\mid C)=P(A\mid C)P(B\mid C)$

\smallskip
\hrule

\smallskip
``$\lim\inf$ and $\lim\sup$ are events.''
$\quad\displaystyle\lim\inf A_k
=\bigcup_{n=1}^\infty\bigcap_{j=n}^\infty A_j$
$\quad\displaystyle\lim\sup A_k
=\bigcap_{n=1}^\infty\bigcup_{j=n}^\infty A_j$

\smallskip
$\cap_{k=1}^\infty A_k=\{\omega:\omega\in A_k\,
\hbox{for all $k$}\}$

$\lim\inf A_k=\{\omega:\omega\in A_k\,
\hbox{for all $k>n$ for some $n$}\}$

$\lim\sup A_k=\{\omega:\omega\in A_k\,
\hbox{for infinitely many $k$}\}$

$\cup_{k=1}^\infty=\{\omega:\omega\in A_k\,
\hbox{for at least one $k$}\}$

\smallskip
\centerline{
$\displaystyle
\bigcap_{k=1}^\infty A_k
\subseteq
\lim\inf A_k
\subseteq
\lim\sup A_k
\subseteq
\bigcup_{k=1}^\infty A_k
$}

\smallskip
$\lim\inf(A_n\cap B_n)=(\lim\inf A_n)\cap(\lim\inf B_n)$
$\quad(\lim\sup A_k)^c=\lim\inf(A_k^c)$

$\lim\sup(A_n\cup B_n)=(\lim\sup A_n)\cup(\lim\sup B_n)$
$\quad\lim\inf A_n\subseteq\lim\inf(A_n\cup B_n)$

\smallskip
Borel-Cantelli lemma:
Let $A_k\in\cal F$ such that
$\displaystyle\sum_{k=1}^\infty P(A_k)<\infty$.
Then $P(\lim\sup A_k)=0$.

\smallskip
i.o. ``infinitely often'' is the sane as $\lim\sup$

\smallskip
Increasing sequence of events:
$A_1\subseteq A_2\subseteq\ldots$ then
$\displaystyle\lim_{i\to\infty}A_i=\bigcup_{i=1}^\infty A_i$

\smallskip
Decreasing sequence of events:
$A_1\supseteq A_2\supseteq\ldots$ then
$\displaystyle\lim_{i\to\infty}A_i=\bigcap_{i=1}^\infty A_i$

\smallskip
Continuity theorem:
$\lim_{k\to\infty}P(A_k)=P(\lim_{k\to\infty}A_k)$

\smallskip
\hrule

\smallskip
$I_A=1-I_{A^c}$
$\quad I_{A\cap B}=I_A\cdot I_B$
$\quad I_{A\cup B}=\max(I_A,I_B)=I_A+I_B-I_AI_B$
$\quad I_A^2=I_A$

\smallskip
\hrule

\smallskip
If $F$ is a distribution function then
(i) $\lim_{x\to-\infty}F(x)=0$,
$\lim_{x\to\infty}F(x)=1$,
(ii) if $x<y$ then $F(x)\le F(y)$,
(iii) $F(x)$ is right continuous,
$\lim_{h\downarrow0}F(x+h)=F(x)$.
Converse is true, any function that satisfies (i), (ii), and (iii)
is a distribution function of some random variable.
If $F(x)\le F(y)$ then $\{\omega:X(\omega)\le x\}
\subseteq\{\omega:Y(\omega)\le y\}$.

\smallskip
\hrule

\smallskip
$X$ is discrete if it can have only countably many values.

\smallskip
\hrule

\smallskip
If $f(x)$ is a probability mass function then
(i) $0\le f(x)\le 1$ for all $x$,
(ii) $\sum_xf(x)=1$.
Converse is true, any function that satisfies (i) and (ii) is a
p.m.f. of some random variable.

\smallskip
\hrule

\smallskip
$f(x)=P(X=x)=F(x)-\lim_{h\downarrow0}F(x-h)$
$\quad F(x)=P(X\le x)=\sum_{y\le x}f(y)$

\smallskip
\hrule

\smallskip
$X$ is a continuous random variable if there is an $f(x)$ such that
(i) $f(x)\ge0$ for all $x$,
(ii) $\displaystyle F(x)=\int_{-\infty}^x f(y)\,dy$.
The function $f(x)$ is called the density of $X$.
Density of a random variable has to satisfy two conditions
(i) $f(x)\ge 0$,
(ii) $\displaystyle\int_{-\infty}^\infty f(x)=1$.
Remark $f(x)=dF/dx$.
``A density is not a probability like mass is.''

\vfill
\eject

$P(X\le x)=F(x)$
$\quad P(X>x)=1-F(X)$
$\quad \displaystyle P(X<x)=\lim_{h\downarrow0}F(x-h)$
$\quad\displaystyle P(X\ge x)=1-\lim_{h\downarrow0}F(x-h)$

\smallskip
$\displaystyle P(a<X<b)=\lim_{h\downarrow0}F(b-h)-F(a)$
$\quad\displaystyle P(a\le X\le b)=F(b)-\lim_{h\downarrow0}F(a-h)$
$\quad\displaystyle P(X=x)=F(x)-\lim_{h\downarrow0}F(x-h)$

\smallskip
\hrule

\smallskip
$\displaystyle\sum_{k=1}^\infty{1\over2^k}=1$
$\quad\displaystyle\sum{1\over k}=\infty$
$\quad\displaystyle\sum_{k=1}^\infty{1\over k^2}={\pi^2\over6}$
$\quad\displaystyle\sum_{k=1}^\infty{1\over k^4}={\pi^4\over90}$
$\quad\displaystyle\sum_{k=1}^nk^2={n(n+1)(2n+1)\over6}$

\smallskip
$\displaystyle\sum_{k=0}^np^k={1-p^{n+1}\over1-p}$
$\quad\displaystyle\sum_{k=0}^\infty p^k={1\over1-p}$
$\quad\displaystyle\sum_{k=1}^\infty p^k={p\over1-p}$
$\quad\displaystyle\sum_{k=1}^\infty{1\over k(k+1)}=1$

\smallskip
$\displaystyle\sum_{k=0}^\infty{x^k\over k!}=e^x$
$\quad\displaystyle\lim_{n\to\infty}{1\over n}
\sum_{k=0}^n\left({k\over n}\right)^m
=\int_0^1x^m\,dx={1\over m+1}$
$\quad\displaystyle{n\choose k}={n!\over k!\,(n-k)!}$

\smallskip
$\log ab=\log a+\log b$
$\quad\log(a/b)=\log a-\log b$
$\quad\log a^b=b\log a$
$\quad\displaystyle\log_2 a={\log a\over\log 2}$
$\quad\displaystyle\lim_{x\to-\infty}\tan^{-1}x=-{\pi\over2}$

\smallskip
$\displaystyle \int e^{ax}={e^{ax}\over a}$
$\quad\displaystyle \int xe^{ax}={e^{ax}(ax-1)\over a^2}$
$\quad\displaystyle \int{du\over a^2+u^2}
={1\over a}\tan^{-1}{u\over a}$
$\quad\displaystyle\int {du\over u}=\log|u|$

\smallskip
\hrule

\smallskip
Expected value $EX=\sum xf(x)$
$\quad Eg(x)=\sum g(x)f(x)$
$\quad E(aX+b)=aEX+b$

\smallskip
Theorem (i) $E1=1$, (ii) $E(aX+bY)=aEX+bEY$,
(iii) if $X\ge0$ then $EX\ge0$.

\smallskip
$\mathop{Var}X=E((X-EX)^2)$
$\quad\mathop{Var}X=E(X^2)-(EX)^2$
$\quad\mathop{Var}(aX+b)=a^2\mathop{Var}X$

\smallskip
If $X$ and $Y$ are independent then $E(XY)=E(X)E(Y)$.
Converse is not true in general.

\smallskip
$\mathop{Cov}(X,Y)=E[(X-EX)(Y-EY)]=E(XY)-E(X)E(Y)\quad$
If $\mathop{Cov}(X,Y)=0$ then $X$ and $Y$ are uncorrelated.

\smallskip
If $X$ and $Y$ are uncorrelated then
$\mathop{Var}(X+Y)=\mathop{Var}X+\mathop{Var}Y$

\smallskip
Correlation coefficient
$\displaystyle \rho(X,Y)={\mathop{Cov}(X,Y)
\over\sqrt{\mathop{Var}(X)\mathop{Var}(Y)}}$

\smallskip
$E(X(X-1))=E(X^2)-EX$
$\quad\mathop{Var}X=EX^2-(EX)^2=E(X(X+1))-EX-(EX)^2$

\smallskip
\hrule

\smallskip
Bernoulli$(p)$ ---
$f(1)$ is the probability of heads, $f(0)$ is the probability
of tails.

Binomial$(n,p)$ ---
$f(k)$ is the probability of $k$ heads in $n$ tosses.

Poisson$(\lambda)$

Geometric$(p)$ ---
$f(k)$ is the probability that it takes $k$ tosses to get a head.

NegativeBinomial$(n,p)$ ---
$f(k)$ is the probability of $k$ tosses to get $n$ heads.

Hypergeometric$(N,b,n)$ ---
Urn with $N$ balls, $b$ are black and $N-b$ are white.
Draw $n$ balls without replacement.
$f(k)$ is the probability of $k$ black balls.

\smallskip
\hrule

\smallskip
Joint distribution function
$F(x,y)=P(X\le x\,\hbox{and}\,Y\le y)\quad$
Joint mass function
$f(x,y)=P(X=x\,\hbox{and}\,Y=y)$

\smallskip
Joined distribution function properties
(i) $\lim_{x,y\to-\infty}F(x,y)=0$,
$\lim_{x,y\to\infty}F(x,y)=1$,
(ii) if $(x_1,y_1)\le(x_2,y_2)$ then $F(x_1,y_1)\le F(x_2,y_2)$,
(iii) continuous from above,
$\lim_{u,v\downarrow0}F(x+u,y+v)=F(x,y)$.
Property (ii) means $F(x,y)$ is nondecreasing.
Another way of putting it is, for any $a_1<a_2$, $b_1<b_2$,
it must be true that
$F(a_2,b_2)-F(a_1,b_2)-F(a_2,b_1)+F(a_1,b_1)\ge0$.
Any function that satisfies (i), (ii), and (iii) is a joined
distribution function.

\smallskip
$\lim_{y\to\infty}F(x,y)=F_X(x)=P(X\le x)$
$\quad\lim_{x\to\infty}F(x,y)=F_Y(y)=P(Y\le y)$

\smallskip
For random vectors, $x\le y$ means that
$x_1\le y_1$ and $x_2\le y_2$.

\smallskip
$F(x,y)$ is jointly continuous if
$\displaystyle F(x,y)=\int_{u=-\infty}^x
\int_{v=-\infty}^y f(u,v)\,du\,dv$

\smallskip
Discrete random variables $X$ and $Y$ are independent iff
$f(x,y)$ can be factored, that is,
$f(x,y)=g(x)h(y)$.

\smallskip
For any intervals $A$ and $B$, if
$P(X\in A,\,Y\in B)=P(X\in A)P(Y\in B)$ then $A$ and $B$
are independent.

\smallskip
Transformation theorem:
If $X$ and $Y$ are independent, then $g(X)$ and $h(Y)$ are
also independent.

\smallskip
$\displaystyle P(X\in A)=\sum_{x\in A}f(x)$

\smallskip
$P(a_1<X\le a_2,\,b_1<Y\le b_2)
=F(a_2,b_2)-F(a_1,b_2)-F(a_2,b_1)+F(a_1,b_1)$

\smallskip
\hrule

\smallskip
``We know that for a decreasing sequence of sets, the limit exists
and it's the empty set.''


\vfill
\end