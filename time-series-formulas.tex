\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,bm,mathrsfs}
\usepackage{graphicx}
\pdfpagewidth 8.5in
\pdfpageheight 11in

\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength\parindent{0in}
\setlength\parskip{0.08in}

\begin{document}



$-1<\theta<1$
$\quad\Rightarrow\quad$
$\displaystyle
\sum_{k=0}^\infty\theta^k={1\over1-\theta}$
$\quad$
$\displaystyle
\sum_{k=1}^\infty\theta^k={\theta\over1-\theta}$
$\quad$
$\displaystyle {d\over dx}\log x={1\over x}$

$\displaystyle
{1\over ad-bc}\begin{pmatrix}
d & -b\\
-c & a
\end{pmatrix}
$
$\quad$
$\displaystyle
{-b\pm\sqrt{b^2-4ac}\over2a}$
$\quad$
Binomial Thm
$\displaystyle
(x+y)^n=\sum_{k=0}^n{n\choose k}x^ky^{n-k}$

${\rm Cov}(X,Y)=E[(X-EX)(Y-EY)]=E(XY)-E(X)E(Y)$

$\gamma(h)={\rm Cov}(X_{t+h},X_t)=E(X_{t+h}X_t)$ when $EX=0$
$\quad$
$\gamma(0)=\mathop{\rm Var}X=\sigma_X^2$

$\displaystyle
\hat\gamma(h)={1\over n}\sum_{t=1}^{n-h}(x_{t+h}-\bar x)(x_t-\bar x)$
$\quad$
$\nabla_{12}X_t=X_t-X_{t-12}$
$\quad$
$\nabla^d=(1-B)^d$

$\cos A\cos B+\sin A\sin B=\cos(A-B)=\cos(B-A)$

$\cos2\lambda=2\cos^2\lambda-1$
$\quad$
$2\cos\theta=e^{i\theta}+e^{-i\theta}$
$\quad$
$2i\sin\theta=e^{i\theta}-e^{-i\theta}$
$\quad$
$(-1)^{|h|}=\cos(\pi h)$

The noise band is $\pm1.96/\sqrt{n}$
$\quad$
Prediction bounds
$\displaystyle\sigma_n^2(h)\approx\sigma^2\sum_{j=0}^{h-1}\psi_j^2$





$\boxed{\hbox{Estimation of $\mu$}}$

$\displaystyle
\sqrt{n}\,(\bar X-\mu)\sim N\left(0,\sum_{h=-\infty}^\infty\gamma(h)\right)$
$\quad$
$\Rightarrow$
$\quad$
$\displaystyle
\bar X\sim N\left(\mu,{1\over n}\sum_{h=-\infty}^\infty\gamma(h)\right)$

The 95\% confidence interval for $\mu$ is $\bar x\pm1.96\sqrt{v/n}$
where $\displaystyle v=\sum_{h=-\infty}^\infty\gamma(h)$

AR(1)$\quad\Rightarrow\quad$
$\displaystyle
v={\sigma^2\over(1-\phi)^2}$
$\quad$
MA(1)$\quad\Rightarrow\quad$
$\displaystyle
v=\sigma^2(1+2\theta+\theta^2)$

%$\displaystyle
%\hat v=\sum_{|h|<\sqrt{n}}\left(1-{|h|\over n}\right)\hat\gamma(h)$


$\boxed{\hbox{Best Linear Predictor}}$
%$\mathbf a_n=\Gamma_n^{-1}\gamma_n(h)$
%$\quad$
%$\displaystyle\Gamma_n=\begin{pmatrix}
%\gamma(0) & \gamma(1) & \cdots & \gamma(n-1)\\
%\gamma(1) & \gamma(0) & \cdots & \gamma(n-2)\\
%\vdots & \vdots & \ddots & \vdots\\
%\gamma(n-1) & \gamma(n-2) & \cdots & \gamma(0)
%\end{pmatrix}
%$
%$\quad$
%$\displaystyle
%\gamma_n(h)=\begin{pmatrix}
%\gamma(h)\\
%\gamma(h+1)\\
%\vdots\\
%\gamma(h+n-1)
%\end{pmatrix}
%$

$\displaystyle
\begin{pmatrix}
a_1\\
a_2
\end{pmatrix}
=
\begin{pmatrix}
\gamma(0) & \gamma(1)\\
\gamma(1) & \gamma(0)
\end{pmatrix}^{-1}
\begin{pmatrix}
\gamma(1)\\
\gamma(2)
\end{pmatrix}
$
$\qquad$
$\displaystyle
a_0=\mu(1-a_1-a_2)
$

$\displaystyle
P_nX_{n+h}=\mu+a_1(X_n-\mu)+\cdots+a_n(X_1-\mu)
=
a_0+a_1X_n+\cdots+a_nX_1
$

$E(X_{n+h}-P_nX_{n+h})^2=\gamma(0)-\mathbf a_n^T\gamma_n(h)$

$E(X_{n+h}-P_nX_{n+h})=0$

$E[(X_{n+h}-P_nX_{n+h})X_j]=0$, $j=1,\ldots,n$

$E(X_{n+1}-\hat X_{n+1})^2=\sigma^2r_n=v_n$


$\boxed{\hbox{Innovations}}$

$v_0=E[(X_1-\hat X_1)^2]=\gamma(0)$
$\quad$
$\displaystyle\theta_{11}={\gamma(1)\over\gamma(0)}$

$\displaystyle v_1=E[(X_2-\hat X_2)^2]=\gamma(0)-{\gamma(1)^2\over\gamma(0)}$
$\quad$
$\displaystyle\theta_{22}={\gamma(2)\over\gamma(0)}$
$\quad$
$\displaystyle\theta_{21}=
{\gamma(1)[\gamma(0)-\gamma(2)]\over\gamma(0)^2-\gamma(1)^2}$
$\quad$
$\displaystyle
\theta_{33}={\gamma(3)\over\gamma(0)}$




$\boxed{\hbox{MA($\infty$)}}$
$\quad$
$\displaystyle
X_t=\sum_{j=0}^\infty\psi_jZ_{t-j}$
$\quad$
$\boxed{\hbox{AR($\infty$)}}$
$\quad$
$\displaystyle
Z_t=\sum_{j=0}^\infty \pi_jX_{t-j}$


$\boxed{\hbox{AR(1)}}
$\quad$
X_t-\phi X_{t-1}=Z_t$
$\quad$
$\{Z_t\}\sim{\rm WN}(0,\sigma^2)$
$\quad$
$\displaystyle
\gamma_X(h)={\sigma^2\phi^{|h|}\over1-\phi^2}$
$\quad$
$\rho_X(h)=\phi^{|h|}$

$\displaystyle
\hat\phi={\hat\gamma(1)\over\hat\gamma(0)}$
$\quad$
$\hat\sigma^2=\hat\gamma(0)-\hat\gamma(1)\hat\phi$
$\quad$
$\displaystyle\bar x\pm1.96{\sigma\over(1-\phi)\sqrt{n}}$
$\quad$
$\displaystyle
E(X_{n+h}-P_nX_{n+h})^2={\sigma^2(1-\phi^{2h})\over1-\phi^2}$

$|\phi|<1$
$\Rightarrow\quad$
$\displaystyle
X_t=\sum_{j=0}^\infty\phi^jZ_{t-j}$
$\quad$
$|\phi|>1$
$\Rightarrow\quad$
$\displaystyle
X_t=-\sum_{j=1}^\infty\phi^{-j}Z_{t+j}$
$\quad$
$\displaystyle
\sum_{h=-\infty}^\infty\gamma_X(h)={\sigma^2\over(1-\phi)^2}$

$\displaystyle
f(\lambda)={\sigma^2\over2\pi}\left(1-2\phi\cos\lambda+\phi^2\right)^{-1}$

$\boxed{\hbox{MA(1)}}$
$\quad$
$X_t=Z_t+\theta Z_{t-1}$
$\quad$
$\{Z_t\}\sim{\rm WN}(0,\sigma^2)$
$\quad$
$\displaystyle
\gamma_X(h)=\left\{\begin{array}{ll}
\sigma^2(1+\theta^2) & h=0\\
\sigma^2\theta & h=\pm1\\
0 & |h|>1
\end{array}\right.$

$\bar x_n\pm1.96\sqrt{v/n}$
$\quad$
$\displaystyle
v=\sum_{h=-\infty}^\infty\gamma(h)=\sigma^2(1+2\theta+\theta^2)$
$\quad$
$\displaystyle
f(\lambda)={\sigma^2\over2\pi}\left(1+2\theta\cos\lambda+\theta^2\right)$



$\boxed{\hbox{ARMA(1,1)}}$

$\psi_0=1$
$\quad$
$\psi_j=(\phi+\theta)\phi^{j-1}$, $j=1,2,\ldots$

$\pi_0=1$
$\quad$
$\pi_j=-(\phi+\theta)(-\theta)^{j-1}$, $j=1,2,\ldots$

$\displaystyle
\gamma(0)=\sigma^2\left[
1+{(\theta+\phi)^2\over1-\phi^2}\right]$
$\quad$
$\displaystyle
\gamma(1)=\sigma^2\left[
\theta+\phi+{(\theta+\phi)^2\phi\over1-\phi^2}\right]$
$\quad$
$\gamma(h)=\phi^{h-1}\gamma(1)$, $h\ge2$

$\boxed{\hbox{ARMA($p$,$q$)}}$

$\phi(z)=1-\phi_1z-\cdots-\phi_pz^p$
$\quad$
Roots of $\phi(z)$ greater than 1 implies causality.

$\theta(z)=1+\theta_1z+\cdots+\theta_qz^q$
$\quad$
Roots of $\theta(z)$ greater than 1 implies invertibility.

$\displaystyle
\psi(z)={\theta(z)\over\phi(z)}=1+\psi_1z+\psi_2z^2+\psi_3z^3+\cdots$

$\displaystyle
\psi_j=\theta_j+\sum_{k=1}^p\phi_k\psi_{j-k}$
$\quad$
$\displaystyle
\pi_j=-\phi_j-\sum_{k=1}^q\theta_k\pi_{j-k}$
$\quad$
$\displaystyle
f(\lambda)={\sigma^2\over2\pi}{\left|\theta\left(e^{-i\lambda}\right)\right|^2
\over\left|\phi\left(e^{-i\lambda}\right)\right|^2}$
$\quad$
$-\pi\le\lambda\le\pi$

$\boxed{\hbox{Yule-Walker}}$
$\quad$
$\begin{pmatrix}
\hat\phi_1\\
\hat\phi_2
\end{pmatrix}
=
\begin{pmatrix}
\hat\gamma(0) & \hat\gamma(1)\\
\hat\gamma(1) & \hat\gamma(0)
\end{pmatrix}^{-1}
\begin{pmatrix}
\hat\gamma(1)\\
\hat\gamma(2)
\end{pmatrix}
$
$\quad$
$\hat\sigma^2=\hat\gamma(0)-\hat\gamma(1)\hat\phi_1-\hat\gamma(2)\hat\phi_2$


\newpage



$\boxed{\hbox{3.2.1 Calculation of the ACVF}}$


$\displaystyle
X_t=\sum_{j=0}^\infty \psi_jZ_{t-j}$, $\{Z_t\}\sim{\rm WN}(0,\sigma^2)$
$\quad\Rightarrow\quad$
$\displaystyle\gamma_X(h)=E(X_{t+h}X_t)=\sigma^2\sum_{j=0}^\infty\psi_j\psi_{j+|h|}$

$\displaystyle\gamma_X(0)=\sigma^2\sum_{j=0}^\infty\psi_j^2$
$\quad$
MA($q$)
$\Rightarrow\quad$
$\displaystyle
\gamma_X(h)=\left\{\begin{array}{ll}
\displaystyle\sigma^2\sum_{j=0}^{q-|h|}\theta_j\theta_{j+|h|} & |h|\le q\\
0 & |h|>q
\end{array}\right.
$



$\boxed{\hbox{4.1 Spectral Densities}}$
$\quad$
WN
$\Rightarrow$
$\displaystyle
f(\lambda)={\sigma^2\over2\pi}$


$\lambda=\hbox{radians}$
$\quad$
$\omega=\hbox{radians/sec}$
$\quad$
$\hbox{period}=2\pi\,\hbox{radians}/\omega=\hbox{seconds}$

$f(\lambda)=\hbox{spectral density function}$
$\quad$
$F(\lambda)=\hbox{spectral distribution function}$

$\displaystyle
f(\lambda)={1\over2\pi}\sum_{h=-\infty}^\infty
e^{-ih\lambda}\gamma(h)
={1\over2\pi}\left[
\gamma(0)+\sum_{h=1}^\infty(e^{-ih\lambda}+e^{ih\lambda})\gamma(h)\right]
$, $-\infty<\lambda<\infty$

$f(\lambda)\ge0$
$\quad$
$f(-\lambda)=f(\lambda)$
$\quad$
$f(\lambda+2\pi)=f(\lambda)$
$\quad$
$\displaystyle\int_{-\pi}^\pi f(\lambda)\,d\lambda=\gamma(0)$

$\displaystyle
\gamma(h)=\int_{-\pi}^\pi e^{ih\lambda}f(\lambda)\,d\lambda
=\int_{-\pi}^\pi\cos(h\lambda)f(\lambda)\,d\lambda$

$\displaystyle
F(\lambda)=\int_{-\pi}^\lambda f(y)\,dy$
$\quad$
$F(-\pi)=0$
$\quad$
$F(\pi)=\gamma(0)$
$\quad$
$d\,F(\lambda)=f(\lambda)\,d\lambda$


$X_t=A\cos(\omega t)+B\sin(\omega t)$;\;
$A,B\sim(0,\nu^2)$;\;
$\gamma_X(h)=\nu^2\cos(\omega h)$;\;
$\displaystyle
F_X(\lambda)=\left\{\begin{array}{ll}
0 & \lambda<-\omega\\
\nu^2/2 & -\omega\le\lambda<\omega\\
\nu^2 & \lambda\ge\omega
\end{array}\right.
$



$\boxed{\hbox{4.3 Time-Invariant Linear Filters}}$

Filter $\displaystyle
Y_t=\sum_{j=-\infty}^\infty\psi_jX_{t-j}$
$\quad\Rightarrow\quad$
$f_Y(\lambda)=\left|\Psi\left(e^{-i\lambda}\right)\right|^2f_X(\lambda)
\quad=\Psi\left(e^{-i\lambda}\right)
\Psi\left(e^{i\lambda}\right)f_X(\lambda)$

Transfer function
$\displaystyle\Psi\left(e^{-i\lambda}\right)=\sum_{j=-\infty}^\infty
\psi_je^{-ij\lambda}$
$\quad$
Power transfer function
$\displaystyle\left|\Psi\left(e^{-i\lambda}\right)\right|^2$

$\displaystyle
Y_t={1\over2q+1}\sum_{j=-q}^qX_{t-j}$
$\quad\Rightarrow\quad$
$\displaystyle
f_Y(\lambda)={1\over2q+1}\left(
\sum_{j=-q}^qe^{-ij\lambda}\right)^2f_X(\lambda)$

$q=1$
$\quad\Rightarrow\quad$
$f_Y(\lambda)={1\over3}\left(e^{i\lambda}+1+e^{-i\lambda}\right)^2f_X(\lambda)$

\newpage


$\boxed{\hbox{5.3 Diagnostic Checking}}$

$\displaystyle
\hat W_t={X_t-\hat X_t\over\sqrt{r_{t-1}}}\quad t=1,\ldots,n$
$\quad$
The noise band is $\pm1.96/\sqrt{n}$





$\boxed{\hbox{5.4 Forecasting}}$
$\quad$
$\displaystyle
P_nX_{n+1}=\phi_1X_n+\cdots+\phi_pX_{n+1-p}
+\sum_{j=1}^q\theta_{nj}(X_{n+1-j}-\hat X_{n+1-j})$
$\quad$
$\theta_{nj}\approx\theta_j$

$X_{n+h}-\hat X_{n+h}=0$ for $h\ge1$
$\quad$
$\displaystyle
v_n=\sigma^2r_n=E[(X_{n+1}-P_nX_{n+1})^2]=
\sigma_n^2(1)\approx\sigma^2\sum_{j=0}^0\psi_j^2=\sigma^2$

$\boxed{\hbox{5.5 Order Selection}}$
$\quad$
${\rm AICC}=-2\log L+2(p+q+1)n/(n-p-q-2)$

$\displaystyle
L={1\over\sqrt{(2\pi)^nv_0\cdots v_{n-1}}}
\exp\left[
-{1\over2}\sum_{j=1}^n{(X_j-\hat X_j)^2\over v_{j-1}}\right]$
%={1\over\sqrt{(2\pi\sigma^2)^nr_0\cdots r_{n-1}}}
%\exp\left[
%-{1\over2\sigma^2}\sum_{j=1}^n{(X_j-\hat X_j)^2\over r_{j-1}}\right]$
$\quad$
$v_n=\sigma^2r_n$

$\displaystyle
\ell=\log L=
-{n\over2}\log(2\pi)
-{n\over2}\log\sigma^2
-{1\over2}\sum_{j=1}^n\log r_{j-1}
-{1\over2\sigma^2}
\sum_{j=1}^n{(X_j-\hat X_j)^2\over r_{j-1}}$

Solve $\displaystyle{\partial\ell\over\partial\sigma^2}=0$
to obtain
$\displaystyle
\hat\sigma^2={1\over n}\sum_{j=1}^n{(X_j-\hat X_j)^2\over r_{j-1}}$

Maximize reduced log likelihood
$\displaystyle
\ell^*=-{n\over2}\log\hat\sigma^2-{1\over2}\sum_{j=1}^n\log r_{j-1}$

Or multiply by $-2/n$ and minimize
$\displaystyle
\ell^*=\log\hat\sigma^2+{1\over n}\sum_{j=1}^n\log r_{j-1}$
$\quad$
(5.2.12) on p.\ 160

$\displaystyle
E(Y_{n+1}-\hat\phi_1Y_n-\cdots-\hat\phi_pY_{n+1-p})^2\approx
\sigma^2\left(1+{p\over n}\right)$



$\boxed{\hbox{Classical Decomposition}}$
$\quad$
$X_t=m_t+s_t+Y_t$
$\quad$
$EY_t=0$
$\quad$
$s_{t+d}=s_t$
$\quad$
$\displaystyle
\sum_{j=1}^ds_j=0$

No distortion
$\Rightarrow\quad$
$\displaystyle
m_t=\sum_ja_jm_{t-j}$
for all polynomials
$m_t=c_0+c_1t+\cdots+c_kt^k$

No distortion
$\Rightarrow\quad$
$\displaystyle
\sum_ja_j=1$
and
$\displaystyle
\sum_jj^ra_j=0$
for $j=1,\ldots,k$

Eliminate seasonal components
$\Rightarrow\quad$
$\displaystyle
\sum_ja_js_{t-j}={\rm const}\times\sum_{j=1}^ds_j=0$



\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Model & ACF & PACF\\
\hline
AR($p$) & Decays & Zero for $h>p$\\
MA($q$) & Zero for $h>q$ & Decays\\
ARMA($p$,$q$) & Decays & Decays\\
\hline
\end{tabular}
\end{center}






\end{document}